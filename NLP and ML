# 1. Introduction

This project explores the GPT-2 language model using Hugging Face's Transformers library. 
We aim to analyze its performance, generation capabilities, and behavior under different conditions.

#2

!pip install transformers wordcloud
from transformers import pipeline
generator = pipeline("text-generation", model="gpt2")

## 3. GPT-2 Overview

GPT-2 is a generative pre-trained transformer developed by OpenAI. 
It predicts the next word in a sequence, enabling it to generate coherent and contextually rich text.

prompt = "In a world where robots rule,"
result = generator(prompt, max_length=50)[0]['generated_text']
print(result)

## 5. Exploration and Analysis

We explore GPT-2â€™s behavior by tweaking generation parameters like temperature, top_k, and prompt complexity.

prompt = "In the future, AI will"
for temp in [0.3, 0.7, 1.0, 1.3]:
    print(f"\n--- Temperature: {temp} ---")
    output = generator(prompt, max_length=60, temperature=temp, do_sample=True)[0]['generated_text']
    print(output)

## 6. Research Questions

- How does temperature affect the creativity and coherence of GPT-2 outputs?
- Can GPT-2 summarize or continue a paragraph meaningfully?
- Does GPT-2 exhibit any biases in its responses?


from wordcloud import WordCloud
import matplotlib.pyplot as plt

text = generator("AI is transforming the world of", max_length=100)[0]['generated_text']
wordcloud = WordCloud(width=800, height=400).generate(text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()


## 8. Ethical Considerations

While GPT-2 is a powerful tool, it can generate biased, inappropriate, or misleading content. 
Ethical use includes prompt filtering, human moderation, and awareness of bias.

## 9. Conclusion and Insight

GPT-2 shows impressive language generation capabilities, but also limitations in logical consistency and fairness. 
Further research could involve fine-tuning on specific domains and bias mitigation techniques.


